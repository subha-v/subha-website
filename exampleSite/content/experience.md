+++
title = "Experience"
description = "Professional experience and research positions"
date = "2025-01-28"
author = "Subha Vadlamannati"
+++

# Experience

---

## Machine Learning Engineer
**VALUENEX** | Palo Alto, CA | June 2025 – September 2025

- Engineered a high-precision indoor positioning system using Apple's Nearby Interaction (UWB) framework, achieving centimeter-level distance accuracy (±10cm) across a distributed mesh network of iOS devices
- Developed a multi-anchor mesh architecture with dual-stack IPv4/IPv6 support, leveraging Bonjour/mDNS for zero-configuration discovery and real-time distance tracking across multiple concurrent nodes

---

## Research Intern
**Stanford Artificial Intelligence Laboratory (SAIL)** | Stanford, CA | June 2024 – January 2025

- Worked with Stanford's SALT lab and CMU's Neulab to develop **HILITE**, an open-source interactive image-editing platform that integrates six state-of-the-art diffusion models (InstructPix2Pix, AnyDoor, etc.) to generalize across diverse editing tasks, including style transfer and object swapping
- Implemented a human-in-the-loop workflow using Next.js, FastAPI, and RunPod serverless GPUs to iteratively collect user feedback and specific hyperparameters, creating a parallel dataset for future VLM fine-tuning

---

## CEO & Founding Fullstack Engineer
**OpenNLP Labs** (formerly Linguistics Justice League) | Seattle, WA | March 2021 – Present

- Developed 3 fullstack web applications (**Edulang**, **Polyglo**, **HeritageHub**) alongside CMU's Neulab & Stanford SAIL used by over 1000+ refugees for resources in 108+ languages
- Collected user feedback & fine-tuned LLMs to improve accessibility
- Secured **$185k+** in funding from Microsoft, T-Mobile, and others
- **Featured on NASDAQ's billboard on Times Square**

---

## Research Intern
**University of California, Santa Barbara** | Santa Barbara, CA | June 2023 – October 2023

- 1 of 77 out of 4000 applicants selected to participate in the Research Mentorship Program (RMP)
- Developed a novel **PTNN (Partially Tensorized Transformers)** approach to compress vision-language models (BERT, ViT) by 53%, improving accuracy by up to 5% without post-training adjustments
- Presented & published findings at MIT's Undergraduate Research Technology Conference & GRITx
